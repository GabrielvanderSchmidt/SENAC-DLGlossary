# Estrutura da Rede

#### Perceptron (Neurônio Artificial):
É o tipo mais simples de rede neural **feedforward**, e hoje é usado como sinônimo para **neurônio artificial**, que é o bloco fundamental de **redes neurais artificiais**. Neurônios artificiais são agrupados lateralmente em **camadas densas**, e estas são empilhadas verticalmente para formar **redes neurais multicamadas**. Consiste de uma soma das entradas ponderadas pelos **pesos/parâmetros** conectados $$u = w_0 \cdot X_0 + … + w_n \cdot X_n + b$$ seguida de uma **função de ativação** não-linear $$g(u)$$. Um neurônio artificial é equivalente a uma regressão logística quando usando **função de ativação sigmóide**.

#### Perceptron Multicamadas (MLP) / Rede Neural Artificial:
É um modelo formado pelo agrupamento de **camadas** de **neurônios artificiais**. O **aprendizado** ocorre através do ajuste dos seus **parâmetros** $$\Theta$$ (pesos $$w$$ e biases $$b$$), otimizados via **descida de gradiente estocástica** (SGD) e **backpropagation** (retropropagação dos erros) conforme uma **função de perda** (loss function, $$\mathcal{L}(\hat{y}, y)$$) a ser minimizada.

#### Função de Ativação:
Proporciona a não-linearidade que **camadas neurais** precisam para poderem **aprender** efetivamente, as quais caso contrário colapsariam em uma combinação linear equivalente a uma única camada. Algumas funções comuns usadas como ativação são:
- **Sigmóide/Logística**: Fórmula: $$\sigma (x) = \frac{1}{1 + e^{-x}}$$. Intervalo da função: $$(0, 1)$$. Intervalo da derivada: $$(0, 0.25]$$. Muito presente no começo de **perceptrons multicamadas**, especialmente por sua saída poder ser interpretada como uma probabilidade (útil em problemas de classificação). Caiu em desuso em favor da **tangente hiperbólica** e depois pela **ReLU** por exacerbar o problema de **vanishing gradients**. Ainda utilizada em aplicações específicas, como problemas de classificação com rótulos/labels não exclusivos (ex.: classificação multilabel).
- **Tangente hiperbólica**: Fórmula: $$tanh (x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$. Intervalo da função: $$(-1, 1)$$. Intervalo da derivada: $$(0, 1]$$. Substituiu a função **sigmóide** como função de ativação padrão em redes neurais por ter várias características desejáveis, como **gradiente** mais íngreme (**convergência** mais rápida), saída centrada em zero (útil antes da adoção de **batch normalization**), e mitigação de **vanishing gradients** (quando $$x$$ próximo a zero/longe das "caudas" da tanh). Embora hoje muito menos comum que a **ReLU**, ainda aparece em certas arquiteturas e problemas específicos, como **redes neurais recursivas** (RNNs).
- **Rectified Linear Unit - ReLU**: Fórmula: $$ReLU (x) = max(0, x)$$. Intervalo da função: $$[0, \infty)$$. Intervalo da derivada: $$[0, 1]$$. Essa função e suas variações se tornaram a função de ativação padrão usada na maioria das redes neurais modernas por mitigarem **vanishing gradients** melhor do que a **tanh**. Isso ocorre porque a tanh tem **regiões de saturação** onde os valores de entrada ($$x$$) são comprimidos para o intervalo $$(-1, 1)$$, causando regiões planas na função (as “caudas” da tanh) onde o **gradiente** é muito próximo de zero (ou seja, vanishing gradients). A ReLU, por sua vez, tem apenas regiões onde o gradiente é exatamente 0 ou 1. Isso faz com a ReLU assuma um papel de **gating** no gradiente, permitindo ($$x > 0$$) ou não ($$x \le 0$$) o fluxo do gradiente para camadas anteriores. Isso, porém, implica na possibilidade de **dying ReLUs**, onde, para todos os **batches** do **conjunto de treino**, a entrada $$x$$ da ReLU é menor que zero, impedindo a propagação do gradiente e consequentemente a "morte" daquele neurônio, pois seus pesos não serão atualizados. Embora aconteça, **técnicas de inicialização de pesos** (ex.: He), ou outras formas de **regularização** da rede fazem com que isso raramente seja um problema na prática. Possui várias variantes, como a **Leaky ReLU**, **[Scaled] Exponential Linear Unit**, **Randomized ReLU**, entre outras.

